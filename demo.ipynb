{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant helper functions.\n",
    "The utils file contains implementations of dataset-specific functions, and the estimators file implements several different estimators including:\n",
    "- InfoNCE\n",
    "- NWJ lower bound\n",
    "- NWJ eval + JS train\n",
    "- Donsker-Varadhan lower bound\n",
    "- SMILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from estimators import estimate_mutual_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the helper function for setting up the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimension of the Gaussian\n",
    "\n",
    "dim = 20\n",
    "device = \"cuda:4\"\n",
    "# define the training procedure\n",
    "\n",
    "CRITICS = {\n",
    "    'separable': SeparableCritic,\n",
    "    'concat': ConcatCritic,\n",
    "}\n",
    "\n",
    "BASELINES = {\n",
    "    'constant': lambda: None,\n",
    "    'unnormalized': lambda: mlp(dim=dim, hidden_dim=512, output_dim=1, layers=2, activation='relu').to(device),\n",
    "    'gaussian': lambda: log_prob_gaussian,\n",
    "}\n",
    "\n",
    "\n",
    "def train_estimator(critic_params, data_params, mi_params, opt_params, **kwargs):\n",
    "    \"\"\"Main training loop that estimates time-varying MI.\"\"\"\n",
    "    # Ground truth rho is only used by conditional critic\n",
    "    critic = CRITICS[mi_params.get('critic', 'separable')](\n",
    "        rho=None, **critic_params).to(device)\n",
    "    baseline = BASELINES[mi_params.get('baseline', 'constant')]()\n",
    "\n",
    "    opt_crit = optim.Adam(critic.parameters(), lr=opt_params['learning_rate'])\n",
    "    if isinstance(baseline, nn.Module):\n",
    "        opt_base = optim.Adam(baseline.parameters(),\n",
    "                              lr=opt_params['learning_rate'])\n",
    "    else:\n",
    "        opt_base = None\n",
    "\n",
    "    def train_step(rho, data_params, mi_params):\n",
    "        # Annoying special case:\n",
    "        # For the true conditional, the critic depends on the true correlation rho,\n",
    "        # so we rebuild the critic at each iteration.\n",
    "        opt_crit.zero_grad()\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_base.zero_grad()\n",
    "\n",
    "        if mi_params['critic'] == 'conditional':\n",
    "            critic_ = CRITICS['conditional'](rho=rho).to(device)\n",
    "        else:\n",
    "            critic_ = critic\n",
    "\n",
    "        x, y = sample_correlated_gaussian(\n",
    "            dim=data_params['dim'], rho=rho, batch_size=data_params['batch_size'], cubic=data_params['cubic'])\n",
    "        mi = estimate_mutual_information(\n",
    "            mi_params['estimator'], x, y, critic_, baseline, mi_params.get('alpha_logit', None), device=\"cuda:4\",\n",
    "            **kwargs)\n",
    "        loss = -mi\n",
    "\n",
    "        loss.backward()\n",
    "        opt_crit.step()\n",
    "        if isinstance(baseline, nn.Module):\n",
    "            opt_base.step()\n",
    "\n",
    "        return mi\n",
    "\n",
    "    # Schedule of correlation over iterations\n",
    "    mis = mi_schedule(opt_params['iterations'])\n",
    "    rhos = mi_to_rho(data_params['dim'], mis)\n",
    "\n",
    "    estimates = []\n",
    "    for i in range(opt_params['iterations']):\n",
    "        mi = train_step(rhos[i], data_params, mi_params)\n",
    "        mi = mi.detach().cpu().numpy()\n",
    "        estimates.append(mi)\n",
    "\n",
    "    return np.array(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the dataset, critic and optimization are listed below. For `cubic` results, set `'cubic': True` in `data_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "    'dim': dim,\n",
    "    'batch_size': 64,\n",
    "    'cubic': None\n",
    "}\n",
    "\n",
    "critic_params = {\n",
    "    'dim': dim,\n",
    "    'layers': 2,\n",
    "    'embed_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "opt_params = {\n",
    "    'iterations': 20000,\n",
    "    'learning_rate': 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training over the methods. Each method should take around 2 mins to run on a single GPU under the current experiment setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train for 20000 steps for each case.\n",
    "\n",
    "mi_numpys = dict()\n",
    "\n",
    "for critic_type in ['separable', 'concat']:\n",
    "    mi_numpys[critic_type] = dict()\n",
    "\n",
    "    for estimator in ['infonce', 'nwj', 'js', 'smile']:\n",
    "        mi_params = dict(estimator=estimator, critic=critic_type, baseline='unnormalized')\n",
    "        mis = train_estimator(critic_params, data_params, mi_params, opt_params)\n",
    "        mi_numpys[critic_type][f'{estimator}'] = mis\n",
    "\n",
    "    estimator = 'smile'\n",
    "    for i, clip in enumerate([1.0, 5.0]):\n",
    "        mi_params = dict(estimator=estimator, critic=critic_type, baseline='unnormalized')\n",
    "        mis = train_estimator(critic_params, data_params, mi_params, opt_params, clip=clip)\n",
    "        mi_numpys[critic_type][f'{estimator}_{clip}'] = mis\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helper functions.\n",
    "\n",
    "def find_name(name):\n",
    "    if 'smile_' in name:\n",
    "        clip = name.split('_')[-1]\n",
    "        return f'SMILE ($\\\\tau = {clip}$)'\n",
    "    else:\n",
    "        return {\n",
    "            'infonce': 'CPC',\n",
    "            'js': 'JS',\n",
    "            'nwj': 'NWJ',\n",
    "            'flow': 'GM (Flow)',\n",
    "            'smile': 'SMILE ($\\\\tau = \\\\infty$)'\n",
    "        }[name]\n",
    "\n",
    "def find_legend(label):\n",
    "    return {'concat': 'Joint critic', 'separable': 'Separable critic'}[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5 of the results, InfoNCE, NWJ, Smile 1.0, 5.0, infty\n",
    "\n",
    "ncols = 5\n",
    "nrows = 1\n",
    "EMA_SPAN = 200\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "mi_true = mi_schedule(opt_params['iterations'])\n",
    "        \n",
    "for i, estimator in enumerate(['infonce', 'nwj']):\n",
    "    key = f'{estimator}'\n",
    "    plt.sca(axs[i])\n",
    "    plt.title(find_name(key), fontsize=18)\n",
    "    for net in ['concat', 'separable']:\n",
    "        mis = mi_numpys[net][key]\n",
    "        p1 = plt.plot(mis, alpha=0.3)[0]\n",
    "        mis_smooth = pd.Series(mis).ewm(span=EMA_SPAN).mean()\n",
    "        plt.plot(mis_smooth, c=p1.get_color(), label=find_legend(net))\n",
    "    plt.ylim(0, 11)\n",
    "    plt.xlim(0, 20000)\n",
    "    plt.plot(mi_true, color='k', label='True MI')\n",
    "    if i == 0:\n",
    "        plt.ylabel('MI (nats)')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.axhline(np.log(64), color='k', ls='--', label='log(bs)')\n",
    "        plt.legend()\n",
    "\n",
    "estimator = 'smile'\n",
    "for i, clip in enumerate([1.0, 5.0, None]):\n",
    "    if clip is None:\n",
    "        key = estimator\n",
    "    else:\n",
    "        key = f'{estimator}_{clip}'\n",
    "\n",
    "    plt.sca(axs[i+2])\n",
    "    plt.title(find_name(key), fontsize=18)\n",
    "    for net in ['concat', 'separable']:\n",
    "        mis = mi_numpys[net][key]\n",
    "        EMA_SPAN = 200\n",
    "        p1 = plt.plot(mis, alpha=0.3)[0]\n",
    "        mis_smooth = pd.Series(mis).ewm(span=EMA_SPAN).mean()\n",
    "        plt.plot(mis_smooth, c=p1.get_color(), label=find_legend(net))\n",
    "    plt.plot(mi_true, color='k', label='True MI')\n",
    "    plt.ylim(0, 11)\n",
    "    plt.xlim(0, 20000)\n",
    "\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, the SMILE estimators have much better performance compared to the alternative methods with clipping.\n",
    "We note that SMILE (infty) is simply a version of DV + JS, where we obtain density ratios from JS and use DV to directly estimate the mutual information. This has much higher variance, and is biased lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
